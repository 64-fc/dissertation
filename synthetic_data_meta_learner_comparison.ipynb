{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor, BaseDRRegressor\n",
    "from causalml.inference.tree import CausalRandomForestRegressor\n",
    "from causalml.metrics import get_cumgain, auuc_score, plot_gain\n",
    "from causalml.dataset import synthetic_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "palette = ['plum', 'g', 'orange', 'r', 'b', 'yellow', 'cyan', 'white']\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, uniform\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from causalml.inference.meta.base import BaseLearner\n",
    "from causalml.inference.meta.utils import (\n",
    "    check_treatment_vector,\n",
    "    check_p_conditions,\n",
    "    convert_pd_to_np,\n",
    ")\n",
    "from causalml.metrics import regression_metrics\n",
    "from causalml.propensity import compute_propensity_score\n",
    "\n",
    "logger = logging.getLogger(\"causalml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRTLearner(BaseLearner):\n",
    "    \"\"\"A parent class for H-learner regressor classes.\n",
    "\n",
    "    A H-learner estimates treatment effects with three machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a H-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (control_effect_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if control_effect_learner is None:\n",
    "            self.model_tau_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_c = control_effect_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_t = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tcontrol_effect_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_tau_c.__repr__(),\n",
    "                self.model_tau_t.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        if p is None:\n",
    "            self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "        self.models_tau_c = {\n",
    "            group: deepcopy(self.model_tau_c) for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau_t = {\n",
    "            group: deepcopy(self.model_tau_t) for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        for group in self.t_groups:\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            X_filt = X[mask]\n",
    "            y_filt = y[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "\n",
    "            # Calculate variances and treatment effects\n",
    "            var_c = (\n",
    "                y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "            ).var()\n",
    "            self.vars_c[group] = var_c\n",
    "            var_t = (\n",
    "                y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "            ).var()\n",
    "            self.vars_t[group] = var_t\n",
    "\n",
    "            # Train treatment models\n",
    "            d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "            d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "            self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "            self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            logger.info(\"Generating propensity score\")\n",
    "            p = dict()\n",
    "            for group in self.t_groups:\n",
    "                p_model = self.propensity_model[group]\n",
    "                p[group] = p_model.predict(X)\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            model_tau_c = self.models_tau_c[group]\n",
    "            model_tau_t = self.models_tau_t[group]\n",
    "            dhat_cs[group] = model_tau_c.predict(X)\n",
    "            dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "            _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "                -1, 1\n",
    "            )\n",
    "            te[:, i] = np.ravel(_te)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                X_filt = X[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "\n",
    "                yhat = np.zeros_like(y, dtype=float)\n",
    "                yhat = self.model_mu.predict(X)\n",
    "\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            dhat_c = dhat_cs[group][mask]\n",
    "            dhat_t = dhat_ts[group][mask]\n",
    "            p_filt = p[group][mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    self.vars_t[group] / prob_treatment\n",
    "                    + self.vars_c[group] / (1 - prob_treatment)\n",
    "                    + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "                )\n",
    "                / w.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "            return ate, ate_lower, ate_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRTRegressor(BaseRTLearner):\n",
    "    \"\"\"\n",
    "    A parent class for H-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize an X-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            control_effect_learner=control_effect_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePLearner(BaseLearner):\n",
    "    \"\"\"A parent class for P-learner regressor classes.\n",
    "\n",
    "    A P-learner estimates treatment effects with one machine learning model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a P-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None)\n",
    "\n",
    "        self.model_nu = deepcopy(learner)\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "        self.distance_threshold = np.inf\n",
    "        self.pairing_method = 'knn'\n",
    "        self.num_neighbours = 10\n",
    "        self.rng = np.random.default_rng()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(pairwise_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_nu.__repr__(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Numerically stable softmax function (using scipy).\"\"\"\n",
    "        if x.size == 0: \n",
    "            return np.array([])\n",
    "        return scipy_softmax(x)\n",
    "\n",
    "    def create_pairs(self, X_np, t_np):\n",
    "        \"\"\"\n",
    "        Creates pairs based on distance in the original feature space using\n",
    "        either 'softmax' sampling or 'knn' selection.\n",
    "        Returns a list of (idx_i, idx_j) tuples.\n",
    "        \"\"\"\n",
    "        n_samples = X_np.shape[0]\n",
    "        idx = np.arange(n_samples)\n",
    "        idx_c = idx[t_np == 0]\n",
    "        idx_t = idx[t_np == 1]\n",
    "\n",
    "        if len(idx_c) == 0 or len(idx_t) == 0:\n",
    "            print(\"Warning: No samples in control or treatment group for pairing.\")\n",
    "            return []\n",
    "\n",
    "        X_c = X_np[idx_c]\n",
    "        X_t = X_np[idx_t]\n",
    "\n",
    "        all_pairs = [] # List of (original_index_i, original_index_j)\n",
    "\n",
    "        # --- Function to process one direction (e.g., target=Treated, pool=Control) ---\n",
    "        def find_pairs_one_direction(target_indices, target_X, pool_indices, pool_X):\n",
    "            local_pairs = []\n",
    "            if len(target_X) == 0 or len(pool_X) == 0:\n",
    "                return local_pairs\n",
    "\n",
    "            distances = cdist(target_X, pool_X, metric='euclidean')\n",
    "\n",
    "            for i in range(len(target_X)): # For each target unit i\n",
    "                dists_i = distances[i, :]\n",
    "                original_target_idx = target_indices[i]\n",
    "\n",
    "                if self.pairing_method == 'softmax':\n",
    "                    # --- Softmax Sampling Logic ---\n",
    "                    potential_neighbour_indices = np.where(dists_i <= self.distance_threshold)[0]\n",
    "                    if len(potential_neighbour_indices) == 0: continue\n",
    "\n",
    "                    valid_distances = dists_i[potential_neighbour_indices]\n",
    "                    neg_distances = -valid_distances\n",
    "                    probs = self._softmax(neg_distances)\n",
    "                    if np.any(np.isnan(probs)) or not np.isclose(np.sum(probs), 1.0): probs = None\n",
    "\n",
    "                    num_to_sample = min(self.num_neighbours, len(potential_neighbour_indices))\n",
    "                    if num_to_sample == 0: continue\n",
    "\n",
    "                    try:\n",
    "                         sampled_local_indices = self.rng.choice(\n",
    "                             potential_neighbour_indices, size=num_to_sample, replace=False, p=probs\n",
    "                         )\n",
    "                    except ValueError as e: # Catch potential issue with p summing slightly off 1.0\n",
    "                         print(f\"Warning: RNG choice error (likely prob sum issue) for sample {original_target_idx}: {e}. Sampling uniformly.\")\n",
    "                         sampled_local_indices = self.rng.choice(\n",
    "                             potential_neighbour_indices, size=num_to_sample, replace=False # No p specified means uniform\n",
    "                         )\n",
    "\n",
    "                    for local_idx in sampled_local_indices:\n",
    "                        original_pool_idx = pool_indices[local_idx]\n",
    "                        local_pairs.append([original_target_idx, original_pool_idx])\n",
    "\n",
    "                elif self.pairing_method == 'knn':\n",
    "                    # --- KNN Logic ---\n",
    "                    # Determine k: number of neighbours to consider\n",
    "                    k = min(self.num_neighbours, len(pool_indices))\n",
    "                    if k == 0: continue\n",
    "\n",
    "                    # Find the indices of the k nearest neighbours in the pool\n",
    "                    knn_local_indices = np.argsort(dists_i)[:k]\n",
    "                    knn_distances = dists_i[knn_local_indices]\n",
    "\n",
    "                    # Filter these k neighbours by the distance threshold\n",
    "                    valid_knn_mask = knn_distances <= self.distance_threshold\n",
    "                    final_knn_local_indices = knn_local_indices[valid_knn_mask]\n",
    "\n",
    "                    # Create pairs with all valid k-nearest neighbours\n",
    "                    for local_idx in final_knn_local_indices:\n",
    "                        original_pool_idx = pool_indices[local_idx]\n",
    "                        local_pairs.append([original_target_idx, original_pool_idx])\n",
    "                else:\n",
    "                    # Should not happen due to __init__ check, but good practice\n",
    "                    raise ValueError(f\"Unknown pairing method: {self.pairing_method}\")\n",
    "\n",
    "            return local_pairs\n",
    "\n",
    "        # --- Find pairs for treated units (target=Treated, pool=Control) ---\n",
    "        pairs_tc = find_pairs_one_direction(idx_t, X_t, idx_c, X_c)\n",
    "        all_pairs.extend(pairs_tc)\n",
    "\n",
    "        # --- Find pairs for control units (target=Control, pool=Treated) ---\n",
    "        pairs_ct = find_pairs_one_direction(idx_c, X_c, idx_t, X_t)\n",
    "        all_pairs.extend(pairs_ct)\n",
    "\n",
    "        return all_pairs\n",
    "\n",
    "    \n",
    "    def pair_data(self, X, treatment, y):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=self.num_neighbours)\n",
    "\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), self.num_neighbours))\n",
    "\n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, self.num_neighbours, axis=0)  # Repeat X0 for 5 times\n",
    "        Y0_expanded = np.repeat(Y0, self.num_neighbours, axis=0)  # Repeat Y0 for 5 times\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p, num_samples=5):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (optional): number of treated samples to pair with each control.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_neighbours = num_samples\n",
    "        \n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        if self.pairing_method == 'softmax':\n",
    "            pair_ids = np.array(self.create_pairs(X, treatment))\n",
    "            X_pair = np.concatenate([X[pair_ids[:, 0]], X[pair_ids[:, 1]]], axis=1)\n",
    "            nu = y[pair_ids[:, 0]] - y[pair_ids[:, 1]]\n",
    "        else:\n",
    "            X_pair, nu = self.pair_data(X, treatment, y)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_nu.fit(X_pair, nu)\n",
    "\n",
    "        # for group in self.t_groups:\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     X_filt = X[mask]\n",
    "        #     y_filt = y[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #     # Calculate variances and treatment effects\n",
    "        #     var_c = (\n",
    "        #         y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "        #     ).var()\n",
    "        #     self.vars_c[group] = var_c\n",
    "        #     var_t = (\n",
    "        #         y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "        #     ).var()\n",
    "        #     self.vars_t[group] = var_t\n",
    "\n",
    "        #     # Train treatment models\n",
    "        #     d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "        #     d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "        #     self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "        #     self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        _te = (self.model_nu.predict(X)).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     model_tau_c = self.models_tau_c[group]\n",
    "        #     model_tau_t = self.models_tau_t[group]\n",
    "        #     dhat_cs[group] = model_tau_c.predict(X)\n",
    "        #     dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "        #     _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "        #         -1, 1\n",
    "        #     )\n",
    "        #     te[:, i] = np.ravel(_te)\n",
    "\n",
    "        #     if (y is not None) and (treatment is not None) and verbose:\n",
    "        #         mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #         treatment_filt = treatment[mask]\n",
    "        #         X_filt = X[mask]\n",
    "        #         y_filt = y[mask]\n",
    "        #         w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #         yhat = np.zeros_like(y, dtype=float)\n",
    "        #         yhat = self.model_mu.predict(X)\n",
    "\n",
    "        #         logger.info(\"Error metrics for group {}\".format(group))\n",
    "        #         regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=5,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.num_neighbours = num_samples\n",
    "        self.fit(X, treatment, y, p)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        num_samples=5,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_neighbours = num_samples\n",
    "\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "        \n",
    "\n",
    "class BasePRegressor(BasePLearner):\n",
    "    \"\"\"\n",
    "    A parent class for P-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize an X-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRPLearner(BaseLearner):\n",
    "    \"\"\"A parent class for PH-learner regressor classes.\n",
    "\n",
    "    A PH-learner estimates treatment effects with two machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a H-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate pair outcome differences\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (pair_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if pair_learner is None:\n",
    "            self.model_nu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_nu = pair_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tpair_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_nu.__repr__()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def pair_data(self, X, treatment, y, num_samples):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=num_samples)\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), num_samples))\n",
    "\n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, num_samples, axis=0)  # Repeat X0 for 5 times\n",
    "        Y0_expanded = np.repeat(Y0, num_samples, axis=0)  # Repeat Y0 for 5 times\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - self.model_mu.predict(X1_sampled) + self.model_mu.predict(X0_expanded) - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, num_samples=5):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int, optional): number of pairs for each control observation\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # if p is None:\n",
    "        #     self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        # Train pair model\n",
    "        X_paired, nu = self.pair_data(X, treatment, y, num_samples)\n",
    "        self.model_nu.fit(X_paired, nu)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X= convert_pd_to_np(X)\n",
    "        X_paired = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "\n",
    "        fhat = self.model_mu.predict(X)\n",
    "        tauhat = self.model_nu.predict(X_paired)\n",
    "\n",
    "        _te = (tauhat).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, fhat\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=5,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, num_samples)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=5,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, fhat = self.fit_predict(\n",
    "                X, treatment, y, num_samples, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRPRegressor(BaseRPLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PH-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a PH-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate paired treatment effects\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            pair_learner=pair_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePDRLearner(BaseLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    PDR-learner estimates treatment effects with machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a PDR-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): model used for all tasks if specific learners are not provided.\n",
    "            control_outcome_learner (optional): model for control outcomes.\n",
    "            treatment_outcome_learner (optional): model for treated outcomes.\n",
    "            treatment_effect_learner (optional): model for treatment effects.\n",
    "            ate_alpha (float, optional): significance level for ATE CI.\n",
    "            control_name (str or int, optional): label for control group.\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (control_outcome_learner is not None)\n",
    "            and (treatment_outcome_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if control_outcome_learner is None:\n",
    "            self.model_mu_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_c = control_outcome_learner\n",
    "\n",
    "        if treatment_outcome_learner is None:\n",
    "            self.model_mu_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_t = treatment_outcome_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(control_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu_c.__repr__(),\n",
    "                self.model_mu_t.__repr__(),\n",
    "                self.model_tau.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, seed=None):\n",
    "        \"\"\"\n",
    "        Fit the PDR-learner with the doubly robust pairwise estimator.\n",
    "\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (np.array or pd.Series): treatment vector.\n",
    "            y (np.array or pd.Series): outcome vector.\n",
    "            p (optional): propensity scores; if None, they are estimated.\n",
    "            seed (int): random seed.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # Use 3-fold cross-fitting\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "        split_indices = [index for _, index in cv.split(y)]\n",
    "\n",
    "        self.models_mu_c = [deepcopy(self.model_mu_c) for _ in range(3)]\n",
    "        self.models_mu_t = {\n",
    "            group: [deepcopy(self.model_mu_t) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau = {\n",
    "            group: [deepcopy(self.model_tau) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        # Initialize propensity score container if not provided\n",
    "        if p is None:\n",
    "            self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups}\n",
    "\n",
    "        # Cross-fit\n",
    "        for ifold in range(3):\n",
    "            treatment_idx = split_indices[ifold]\n",
    "            outcome_idx = split_indices[(ifold + 1) % 3]\n",
    "            tau_idx = split_indices[(ifold + 2) % 3]\n",
    "\n",
    "            treatment_treat = treatment[treatment_idx]\n",
    "            treatment_out = treatment[outcome_idx]\n",
    "            treatment_tau = treatment[tau_idx]\n",
    "\n",
    "            y_out, y_tau = y[outcome_idx], y[tau_idx]\n",
    "            X_treat, X_out, X_tau = X[treatment_idx], X[outcome_idx], X[tau_idx]\n",
    "\n",
    "            # Propensity score estimation if not provided\n",
    "            if p is None:\n",
    "                logger.info(\"Estimating propensity scores\")\n",
    "                cur_p = dict()\n",
    "                for group in self.t_groups:\n",
    "                    mask = (treatment_treat == group) | (treatment_treat == self.control_name)\n",
    "                    X_filt = X_treat[mask]\n",
    "                    treatment_filt = treatment_treat[mask]\n",
    "                    w_filt = (treatment_filt == group).astype(int)\n",
    "                    # Compute propensity scores for group 'group'\n",
    "                    cur_p[group], _ = compute_propensity_score(\n",
    "                        X=X_filt, treatment=w_filt, X_pred=X_tau, treatment_pred=(treatment_tau == group).astype(int)\n",
    "                    )\n",
    "                    self.propensity[group][tau_idx] = cur_p[group]\n",
    "            else:\n",
    "                cur_p = dict()\n",
    "                if isinstance(p, (np.ndarray, pd.Series)):\n",
    "                    cur_p = {self.t_groups[0]: convert_pd_to_np(p[tau_idx])}\n",
    "                else:\n",
    "                    cur_p = {g: p[g][tau_idx] for g in self.t_groups}\n",
    "                check_p_conditions(cur_p, self.t_groups)\n",
    "\n",
    "            # Outcome regression: fit control and treatment models on outcome fold\n",
    "            logger.info(\"Fitting outcome regressions\")\n",
    "            # Fit control outcome model on control units\n",
    "            self.models_mu_c[ifold].fit(\n",
    "                X_out[treatment_out == self.control_name],\n",
    "                y_out[treatment_out == self.control_name],\n",
    "            )\n",
    "            for group in self.t_groups:\n",
    "                # Fit treated outcome model for each group\n",
    "                self.models_mu_t[group][ifold].fit(\n",
    "                    X_out[treatment_out == group],\n",
    "                    y_out[treatment_out == group],\n",
    "                )\n",
    "\n",
    "            # Fit pseudo outcomes for treatment effect using doubly robust pairwise estimator\n",
    "            logger.info(\"Fitting doubly robust pairwise pseudo outcomes\")\n",
    "            for group in self.t_groups:\n",
    "                # Filter tau-fold: keep observations from control or group 'group'\n",
    "                mask = (treatment_tau == group) | (treatment_tau == self.control_name)\n",
    "                X_filt = X_tau[mask]\n",
    "                y_filt = y_tau[mask]\n",
    "                p_filt = cur_p[group][mask]\n",
    "                # Predict outcomes using outcome models\n",
    "                mu_c_all = self.models_mu_c[ifold].predict(X_filt)\n",
    "                mu_t_all = self.models_mu_t[group][ifold].predict(X_filt)\n",
    "\n",
    "                # Separate into control and treated subsets for pairing\n",
    "                mask_control = (treatment_tau[mask] == self.control_name)\n",
    "                mask_treated = (treatment_tau[mask] == group)\n",
    "                if np.sum(mask_control) == 0 or np.sum(mask_treated) == 0:\n",
    "                    logger.warning(\"Not enough data for pairing in group {}\".format(group))\n",
    "                    continue\n",
    "\n",
    "                X_control = X_filt[mask_control]\n",
    "                y_control = y_filt[mask_control]\n",
    "                p_control = p_filt[mask_control]\n",
    "                mu_control = mu_c_all[mask_control]\n",
    "\n",
    "                X_treated = X_filt[mask_treated]\n",
    "                y_treated = y_filt[mask_treated]\n",
    "                p_treated = p_filt[mask_treated]\n",
    "                mu_treated = mu_t_all[mask_treated]\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "\n",
    "                X_control_scaled = scaler.fit_transform(X_control)\n",
    "                X_treated_scaled = scaler.fit_transform(X_treated)\n",
    "\n",
    "                # Build KDTree on the control group\n",
    "                tree = cKDTree(X_control_scaled)\n",
    "\n",
    "                k = 10\n",
    "\n",
    "                # Find the nearest control neighbour for each treated sample\n",
    "                distances, idx_control = tree.query(X_treated_scaled, k=k)  # k=10\n",
    "\n",
    "                # Expand treated indices to match the number of pairs (each treated unit repeats k times)\n",
    "                idx_treated = np.repeat(np.arange(len(X_treated)), k)\n",
    "                idx_control = idx_control.flatten()  # Flatten to align with repeated treated indices\n",
    "\n",
    "\n",
    "                # Compute the doubly robust pairwise pseudo outcome for each pair:\n",
    "                #   dr_pair = [mu_treated - mu_control] +\n",
    "                #             [ (y_treated - mu_treated) / p_treated - (y_control - mu_control) / (1-p_control) ]\n",
    "                dr_pairs = (\n",
    "                    (mu_treated[idx_treated] - mu_control[idx_control])\n",
    "                    + ((y_treated[idx_treated] - mu_treated[idx_treated]) / p_treated[idx_treated]\n",
    "                       - (y_control[idx_control] - mu_control[idx_control]) / (1 - p_control[idx_control]))\n",
    "                )\n",
    "                # Combine the paired features. Here we simply concatenate the control and treated features.\n",
    "                X_pairs = np.hstack([X_control[idx_control], X_treated[idx_treated]])\n",
    "                # Fit the treatment effect learner on the paired data and doubly robust pseudo outcomes.\n",
    "                self.models_tau[group][ifold].fit(X_pairs, dr_pairs)\n",
    "\n",
    "    def predict(self, X, treatment=None, y=None, p=None, return_components=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Predict treatment effects.\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (optional): treatment vector.\n",
    "            y (optional): outcome vector.\n",
    "            return_components (bool): if True, return predicted outcomes for control and treated separately.\n",
    "            verbose (bool): whether to output logs.\n",
    "        Returns:\n",
    "            np.array: predicted treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        te = np.zeros((X.shape[0], len(self.t_groups)))\n",
    "        yhat_cs = {}\n",
    "        yhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            # Average the treatment effect predictions from the cross-fit models\n",
    "            models_tau = self.models_tau[group]\n",
    "            _te = np.r_[[model.predict(np.hstack([X, X])) for model in models_tau]].mean(axis=0)\n",
    "            te[:, i] = np.ravel(_te)\n",
    "            yhat_cs[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_c]\n",
    "            ].mean(axis=0)\n",
    "            yhat_ts[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_t[group]]\n",
    "            ].mean(axis=0)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "                yhat = np.zeros_like(y_filt, dtype=float)\n",
    "                yhat[w == 0] = yhat_cs[group][mask][w == 0]\n",
    "                yhat[w == 1] = yhat_ts[group][mask][w == 1]\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y_filt, yhat, w)\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, yhat_cs, yhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "            seed (int): random seed for cross-fitting\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, seed)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "\n",
    "        check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        seed=None,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            seed (int): random seed for cross-fitting\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            te, yhat_cs, yhat_ts = self.predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        else:\n",
    "            te, yhat_cs, yhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True, seed=seed\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            yhat_c = yhat_cs[group][mask]\n",
    "            yhat_t = yhat_ts[group][mask]\n",
    "            y_filt = y[mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    (y_filt[w == 0] - yhat_c[w == 0]).var() / (1 - prob_treatment)\n",
    "                    + (y_filt[w == 1] - yhat_t[w == 1]).var() / prob_treatment\n",
    "                    + (yhat_t - yhat_c).var()\n",
    "                )\n",
    "                / y_filt.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(\n",
    "                    X, treatment, y, p, size=bootstrap_size, seed=seed\n",
    "                )\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "\n",
    "class BasePDRRegressor(BasePDRLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            control_outcome_learner=control_outcome_learner,\n",
    "            treatment_outcome_learner=treatment_outcome_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # For potential validation split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# --- Helper Function to Build MLP ---\n",
    "def _build_mlp(input_dim, hidden_dims, output_dim=1, activation=nn.ReLU):\n",
    "    \"\"\"Builds a simple MLP.\"\"\"\n",
    "    layers = []\n",
    "    last_dim = input_dim\n",
    "    for hidden_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "        # Maybe add Dropout here: layers.append(nn.Dropout(p=0.5))\n",
    "        last_dim = hidden_dim\n",
    "    layers.append(nn.Linear(last_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# --- Custom Dataset for Pairs ---\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset to serve pairs of indices.\"\"\"\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs # List of tuples (idx_i, idx_j)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the pair of original indices\n",
    "        return self.pairs[idx]\n",
    "\n",
    "# --- PairNet Implementation with PyTorch ---\n",
    "class PairNetTorch:\n",
    "    \"\"\"\n",
    "    PairNet implementation using PyTorch for explicit pairwise loss optimization.\n",
    "\n",
    "    Uses a T-Learner architecture (separate networks for control and treatment)\n",
    "    trained jointly with the loss: ((y_i - y_j) - (mu(xi, ti) - mu(xj, tj)))^2\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dims: list = [64, 32],\n",
    "                 activation = nn.ReLU,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 epochs: int = 50,\n",
    "                 batch_size: int = 128,\n",
    "                 distance_threshold: float = np.inf,\n",
    "                 num_neighbours: int = 10, # k for KNN, num_samples for softmax\n",
    "                 pairing_method: str = 'softmax', # 'softmax' or 'knn'\n",
    "                 val_size: float = 0.0, # Set > 0 to use early stopping based on validation loss\n",
    "                 patience: int = 5,     # Early stopping patience\n",
    "                 verbose: int = 10,     # Print loss every `verbose` epochs\n",
    "                 random_state: int | None = None,\n",
    "                 device: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the PairNetTorch learner.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of features in X.\n",
    "            hidden_dims (list, optional): List of hidden layer sizes for mu_c and mu_t.\n",
    "                                          Defaults to [64, 32].\n",
    "            activation (torch.nn.Module, optional): Activation function. Defaults to nn.ReLU.\n",
    "            learning_rate (float, optional): Optimizer learning rate. Defaults to 1e-3.\n",
    "            weight_decay (float, optional): L2 regularization strength. Defaults to 1e-4.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 50.\n",
    "            batch_size (int, optional): Batch size for training pairs. Defaults to 128.\n",
    "            distance_threshold (float, optional): Max distance for pairing. Defaults to np.inf.\n",
    "            num_neighbours (int, optional): neighbours per sample in pairing. Defaults to 10.\n",
    "            val_size (float, optional): Fraction of data to use for validation/early stopping.\n",
    "                                        If 0, no validation/early stopping is used. Defaults to 0.0.\n",
    "            patience (int, optional): Early stopping patience (epochs without improvement). Defaults to 5.\n",
    "            verbose (int, optional): Print loss frequency (epochs). Defaults to 10.\n",
    "            random_state (int | None, optional): Random seed. Defaults to None.\n",
    "            device (str | None, optional): Device ('cpu', 'cuda'). Autodetects if None.\n",
    "        \"\"\"\n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.num_neighbours = num_neighbours\n",
    "        self.pairing_method = pairing_method\n",
    "        self.val_size = val_size\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Initialize models, optimizers, scaler\n",
    "        self.mu_c = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "        self.mu_t = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "\n",
    "        # Combine parameters for a single optimizer pass\n",
    "        all_params = list(self.mu_c.parameters()) + list(self.mu_t.parameters())\n",
    "        self.optimizer = optim.Adam(all_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        self.scaler = StandardScaler() # Scale features for NN stability\n",
    "\n",
    "        # Placeholders for data and results\n",
    "        self.X_tensor = None\n",
    "        self.t_tensor = None\n",
    "        self.y_tensor = None\n",
    "        self.pairs_train = []\n",
    "        self.pairs_val = []\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "        self.best_mu_c_state = None\n",
    "        self.best_mu_t_state = None\n",
    "\n",
    "\n",
    "    def _softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Numerically stable softmax function (using scipy).\"\"\"\n",
    "        if x.size == 0: return np.array([])\n",
    "        # Scipy's softmax handles potential numerical issues better\n",
    "        return scipy_softmax(x)\n",
    "\n",
    "    def _create_pairs_torch(self, X_np: np.ndarray, t_np: np.ndarray) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Creates pairs based on distance in the original feature space using\n",
    "        either 'softmax' sampling or 'knn' selection.\n",
    "        Returns a list of (idx_i, idx_j) tuples.\n",
    "        \"\"\"\n",
    "        n_samples = X_np.shape[0]\n",
    "        idx = np.arange(n_samples)\n",
    "        idx_c = idx[t_np == 0]\n",
    "        idx_t = idx[t_np == 1]\n",
    "\n",
    "        if len(idx_c) == 0 or len(idx_t) == 0:\n",
    "            print(\"Warning: No samples in control or treatment group for pairing.\")\n",
    "            return []\n",
    "\n",
    "        X_c = X_np[idx_c]\n",
    "        X_t = X_np[idx_t]\n",
    "\n",
    "        all_pairs = [] # List of (original_index_i, original_index_j)\n",
    "\n",
    "        # --- Function to process one direction (e.g., target=Treated, pool=Control) ---\n",
    "        def find_pairs_one_direction(target_indices, target_X, pool_indices, pool_X):\n",
    "            local_pairs = []\n",
    "            if len(target_X) == 0 or len(pool_X) == 0:\n",
    "                return local_pairs\n",
    "\n",
    "            distances = cdist(target_X, pool_X, metric='euclidean')\n",
    "\n",
    "            for i in range(len(target_X)): # For each target unit i\n",
    "                dists_i = distances[i, :]\n",
    "                original_target_idx = target_indices[i]\n",
    "\n",
    "                if self.pairing_method == 'softmax':\n",
    "                    # --- Softmax Sampling Logic ---\n",
    "                    potential_neighbour_indices = np.where(dists_i <= self.distance_threshold)[0]\n",
    "                    if len(potential_neighbour_indices) == 0: continue\n",
    "\n",
    "                    valid_distances = dists_i[potential_neighbour_indices]\n",
    "                    neg_distances = -valid_distances\n",
    "                    probs = self._softmax(neg_distances)\n",
    "                    if np.any(np.isnan(probs)) or not np.isclose(np.sum(probs), 1.0): probs = None\n",
    "\n",
    "                    num_to_sample = min(self.num_neighbours, len(potential_neighbour_indices))\n",
    "                    if num_to_sample == 0: continue\n",
    "\n",
    "                    try:\n",
    "                         sampled_local_indices = self.rng.choice(\n",
    "                             potential_neighbour_indices, size=num_to_sample, replace=False, p=probs\n",
    "                         )\n",
    "                    except ValueError as e: # Catch potential issue with p summing slightly off 1.0\n",
    "                         print(f\"Warning: RNG choice error (likely prob sum issue) for sample {original_target_idx}: {e}. Sampling uniformly.\")\n",
    "                         sampled_local_indices = self.rng.choice(\n",
    "                             potential_neighbour_indices, size=num_to_sample, replace=False # No p specified means uniform\n",
    "                         )\n",
    "\n",
    "\n",
    "                    for local_idx in sampled_local_indices:\n",
    "                        original_pool_idx = pool_indices[local_idx]\n",
    "                        local_pairs.append((original_target_idx, original_pool_idx))\n",
    "\n",
    "                elif self.pairing_method == 'knn':\n",
    "                    # --- KNN Logic ---\n",
    "                    # Determine k: number of neighbours to consider\n",
    "                    k = min(self.num_neighbours, len(pool_indices))\n",
    "                    if k == 0: continue\n",
    "\n",
    "                    # Find the indices of the k nearest neighbours in the pool\n",
    "                    knn_local_indices = np.argsort(dists_i)[:k]\n",
    "                    knn_distances = dists_i[knn_local_indices]\n",
    "\n",
    "                    # Filter these k neighbours by the distance threshold\n",
    "                    valid_knn_mask = knn_distances <= self.distance_threshold\n",
    "                    final_knn_local_indices = knn_local_indices[valid_knn_mask]\n",
    "\n",
    "                    # Create pairs with all valid k-nearest neighbours\n",
    "                    for local_idx in final_knn_local_indices:\n",
    "                        original_pool_idx = pool_indices[local_idx]\n",
    "                        local_pairs.append((original_target_idx, original_pool_idx))\n",
    "                else:\n",
    "                    # Should not happen due to __init__ check, but good practice\n",
    "                    raise ValueError(f\"Unknown pairing method: {self.pairing_method}\")\n",
    "\n",
    "            return local_pairs\n",
    "\n",
    "        # --- Find pairs for treated units (target=Treated, pool=Control) ---\n",
    "        pairs_tc = find_pairs_one_direction(idx_t, X_t, idx_c, X_c)\n",
    "        all_pairs.extend(pairs_tc)\n",
    "\n",
    "        # --- Find pairs for control units (target=Control, pool=Treated) ---\n",
    "        pairs_ct = find_pairs_one_direction(idx_c, X_c, idx_t, X_t)\n",
    "        all_pairs.extend(pairs_ct)\n",
    "\n",
    "        # Optional: Remove duplicate pairs (e.g., if (a, b) and (b, a) were both added)\n",
    "        # Using frozenset to handle order invariance for uniqueness check\n",
    "        unique_pairs_set = {frozenset(pair) for pair in all_pairs}\n",
    "        # Convert back to list of tuples, preserving an arbitrary order for each unique pair\n",
    "        # Note: This might slightly change the number of pairs compared to just extending lists\n",
    "        final_pairs = [tuple(pair_set) for pair_set in unique_pairs_set]\n",
    "\n",
    "\n",
    "        # print(f\"Created {len(all_pairs)} raw pairs.\") # Before deduplication\n",
    "        #print(f\"Created {len(final_pairs)} unique pairs for training/validation.\")\n",
    "        # return all_pairs # Return raw pairs if duplicates are desired\n",
    "        return final_pairs # Return unique pairs\n",
    "\n",
    "    def _get_predictions_for_pair(self, x_i, t_i, x_j, t_j):\n",
    "        \"\"\"Gets mu(x,t) predictions for both elements of pairs in a batch.\"\"\"\n",
    "        # Predict all i's and j's under both control and treatment scenarios\n",
    "        pred_i_c = self.mu_c(x_i)\n",
    "        pred_i_t = self.mu_t(x_i)\n",
    "        pred_j_c = self.mu_c(x_j)\n",
    "        pred_j_t = self.mu_t(x_j)\n",
    "\n",
    "        # Select the correct prediction based on actual treatment\n",
    "        # t_i and t_j need to have the same shape as predictions for torch.where\n",
    "        # They should be [batch_size, 1]\n",
    "        t_i_mask = t_i.bool() # Convert to boolean mask\n",
    "        t_j_mask = t_j.bool()\n",
    "\n",
    "        pred_i = torch.where(t_i_mask, pred_i_t, pred_i_c)\n",
    "        pred_j = torch.where(t_j_mask, pred_j_t, pred_j_c)\n",
    "\n",
    "        return pred_i, pred_j\n",
    "\n",
    "    def _calculate_loss(self, idx_i_batch, idx_j_batch):\n",
    "        \"\"\"Calculates the pairwise loss for a batch of pairs.\"\"\"\n",
    "        # Retrieve data for the batch using indices\n",
    "        x_i = self.X_tensor[idx_i_batch]\n",
    "        t_i = self.t_tensor[idx_i_batch]\n",
    "        y_i = self.y_tensor[idx_i_batch]\n",
    "\n",
    "        x_j = self.X_tensor[idx_j_batch]\n",
    "        t_j = self.t_tensor[idx_j_batch]\n",
    "        y_j = self.y_tensor[idx_j_batch]\n",
    "\n",
    "        # Get predictions using the T-learner structure\n",
    "        pred_i, pred_j = self._get_predictions_for_pair(x_i, t_i, x_j, t_j)\n",
    "\n",
    "        # Calculate the pairwise loss\n",
    "        true_diff = y_i - y_j\n",
    "        pred_diff = pred_i - pred_j\n",
    "        loss = torch.mean((true_diff - pred_diff)**2) # Mean Squared Error of the differences\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, treatment: pd.Series, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Fit the PairNetTorch model using the pairwise loss.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix.\n",
    "            treatment (pd.Series): Treatment assignment vector (0 or 1).\n",
    "            y (pd.Series): Outcome vector.\n",
    "        \"\"\"\n",
    "        # --- 1. Preprocessing and Data Setup ---\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X_np = self.scaler.fit_transform(X).astype(np.float32)\n",
    "        t_np = treatment.astype(np.float32).reshape(-1, 1) # Ensure shape [n, 1]\n",
    "        y_np = y.astype(np.float32).reshape(-1, 1)       # Ensure shape [n, 1]\n",
    "\n",
    "        self.X_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "        self.t_tensor = torch.tensor(t_np, dtype=torch.float32).to(self.device)\n",
    "        self.y_tensor = torch.tensor(y_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # --- 2. Create Pairs ---\n",
    "        all_pairs = self._create_pairs_torch(X_np, t_np.flatten()) # Use original X for pairing\n",
    "        if not all_pairs:\n",
    "            raise ValueError(\"No pairs were created. Cannot train the model.\")\n",
    "\n",
    "        # --- 3. Split pairs for Train/Validation (if val_size > 0) ---\n",
    "        if self.val_size > 0:\n",
    "            if len(all_pairs) < 2 / self.val_size: # Ensure enough pairs for split\n",
    "                 print(\"Warning: Not enough pairs for validation split, disabling validation.\")\n",
    "                 self.val_size = 0\n",
    "                 self.pairs_train = all_pairs\n",
    "                 self.pairs_val = []\n",
    "            else:\n",
    "                 pairs_train_idx, pairs_val_idx = train_test_split(\n",
    "                    np.arange(len(all_pairs)), test_size=self.val_size, random_state=self.random_state\n",
    "                 )\n",
    "                 self.pairs_train = [all_pairs[i] for i in pairs_train_idx]\n",
    "                 self.pairs_val = [all_pairs[i] for i in pairs_val_idx]\n",
    "                 # print(f\"Using {len(self.pairs_train)} pairs for training, {len(self.pairs_val)} for validation.\")\n",
    "        else:\n",
    "             self.pairs_train = all_pairs\n",
    "             self.pairs_val = []\n",
    "\n",
    "        train_dataset = PairDataset(self.pairs_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        if self.pairs_val:\n",
    "            val_dataset = PairDataset(self.pairs_val)\n",
    "            # Use larger batch size for validation as no gradients are needed\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size * 2)\n",
    "\n",
    "\n",
    "        # --- 4. Training Loop ---\n",
    "        self.mu_c.train()\n",
    "        self.mu_t.train()\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_train_loss = 0.0\n",
    "            for i, (idx_i_batch, idx_j_batch) in enumerate(train_loader):\n",
    "                # Indices need to be tensors for indexing other tensors\n",
    "                idx_i_batch = idx_i_batch.long()\n",
    "                idx_j_batch = idx_j_batch.long()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "\n",
    "            # --- 5. Validation and Early Stopping ---\n",
    "            avg_epoch_val_loss = -1 # Sentinel value if no validation\n",
    "            if self.pairs_val:\n",
    "                self.mu_c.eval()\n",
    "                self.mu_t.eval()\n",
    "                epoch_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for idx_i_batch, idx_j_batch in val_loader:\n",
    "                        idx_i_batch = idx_i_batch.long()\n",
    "                        idx_j_batch = idx_j_batch.long()\n",
    "                        loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                        epoch_val_loss += loss.item()\n",
    "                avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "\n",
    "                if avg_epoch_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = avg_epoch_val_loss\n",
    "                    self.epochs_no_improve = 0\n",
    "                    # Store the best model state\n",
    "                    self.best_mu_c_state = self.mu_c.state_dict()\n",
    "                    self.best_mu_t_state = self.mu_t.state_dict()\n",
    "                else:\n",
    "                    self.epochs_no_improve += 1\n",
    "\n",
    "                # Set back to train mode\n",
    "                self.mu_c.train()\n",
    "                self.mu_t.train()\n",
    "\n",
    "            if self.verbose > 0 and (epoch + 1) % self.verbose == 0:\n",
    "                if self.pairs_val:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "            if self.val_size > 0 and self.epochs_no_improve >= self.patience:\n",
    "                print(f\"Early stopping triggered after epoch {epoch+1}. Best Val Loss: {self.best_val_loss:.4f}\")\n",
    "                # Load the best model state before exiting\n",
    "                if self.best_mu_c_state and self.best_mu_t_state:\n",
    "                    self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "                    self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "                break\n",
    "\n",
    "        # If not early stopping or if early stopping happened on last epoch,\n",
    "        # potentially load the best model state if validation was used.\n",
    "        if self.val_size > 0 and self.best_mu_c_state and self.best_mu_t_state:\n",
    "             self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "             self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, return_components: bool = False) -> np.ndarray | tuple:\n",
    "        \"\"\"\n",
    "        Predict Conditional Average Treatment Effect (CATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for prediction.\n",
    "            return_components (bool, optional): If True, return tuple of\n",
    "                                                (cate, y_pred_c, y_pred_t).\n",
    "                                                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or tuple: Predicted CATE = mu_t(X) - mu_c(X)\n",
    "                                 (or tuple if return_components is True).\n",
    "        \"\"\"\n",
    "        if self.X_tensor is None: # Basic check if fit has been called\n",
    "            raise RuntimeError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "\n",
    "        self.mu_c.eval()\n",
    "        self.mu_t.eval()\n",
    "\n",
    "        X = convert_pd_to_np(X)\n",
    "        X_np = self.scaler.transform(X).astype(np.float32) # Use fitted scaler\n",
    "        X_pred_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_c = self.mu_c(X_pred_tensor)\n",
    "            y_pred_t = self.mu_t(X_pred_tensor)\n",
    "\n",
    "        cate_pred = y_pred_t - y_pred_c\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        cate_pred_np = cate_pred.cpu().numpy()\n",
    "        if return_components:\n",
    "            y_pred_c_np = y_pred_c.cpu().numpy()\n",
    "            y_pred_t_np = y_pred_t.cpu().numpy()\n",
    "            return cate_pred_np, y_pred_c_np, y_pred_t_np\n",
    "        else:\n",
    "            return cate_pred_np\n",
    "\n",
    "    def estimate_ate(self, X: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for ATE estimation.\n",
    "\n",
    "        Returns:\n",
    "            float: Estimated ATE.\n",
    "        \"\"\"\n",
    "        cate_pred = self.predict(X, return_components=False)\n",
    "        return float(np.mean(cate_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE (for generated data): 3.2554\n",
      "Using device: cpu\n",
      "\n",
      "Fitting PairNetTorch...\n",
      "Created 19520 pairs for training/validation.\n",
      "Using 16592 pairs for training, 2928 for validation.\n",
      "Epoch [10/100], Train Loss: 0.8432, Val Loss: 0.8024\n",
      "Epoch [20/100], Train Loss: 0.3914, Val Loss: 0.3770\n",
      "Epoch [30/100], Train Loss: 0.1941, Val Loss: 0.1854\n",
      "Epoch [40/100], Train Loss: 0.1452, Val Loss: 0.1365\n",
      "Epoch [50/100], Train Loss: 0.1110, Val Loss: 0.1029\n",
      "Epoch [60/100], Train Loss: 0.0939, Val Loss: 0.0883\n",
      "Epoch [70/100], Train Loss: 0.0901, Val Loss: 0.0866\n",
      "Epoch [80/100], Train Loss: 0.0887, Val Loss: 0.0853\n",
      "Epoch [90/100], Train Loss: 0.0879, Val Loss: 0.0850\n",
      "Epoch [100/100], Train Loss: 0.0875, Val Loss: 0.0845\n",
      "Loading best model state based on validation loss.\n",
      "Fitting complete.\n",
      "\n",
      "Predicting CATE on test set...\n",
      "Predicted CATE shape: (3000, 1)\n",
      "Predicted CATE for first 5 test samples:\n",
      "[4.621412  3.8907404 4.0869474 3.3348691 3.3718457]\n",
      "CATE Prediction R2 score: 0.9807\n",
      "CATE Prediction MAE: 0.0903\n",
      "\n",
      "Estimating ATE on test set...\n",
      "Estimated ATE: 3.2443\n",
      "True ATE:      3.2448\n",
      "ATE Error:     0.0005\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Generate Sample Data ---\n",
    "n_samples = 1000 # More samples often needed for NNs\n",
    "n_features = 10\n",
    "X = pd.DataFrame(np.random.rand(n_samples, n_features), columns=[f'feat_{i}' for i in range(n_features)])\n",
    "# Treatment assignment (maybe slightly imbalanced)\n",
    "propensity = 1 / (1 + np.exp(-(X['feat_0'] - 0.5))) # Treatment depends on feat_0\n",
    "treatment = pd.Series(np.random.binomial(1, propensity, size=n_samples), name='treatment')\n",
    "\n",
    "# Outcome model: Non-linear effects + interaction\n",
    "y = 1 + np.sin(X['feat_1']*np.pi) + 0.5 * X['feat_2']**2 \\\n",
    "    + treatment * (2.5 + np.cos(X['feat_0']*np.pi) + 1.5*X['feat_3']) \\\n",
    "    + np.random.randn(n_samples) * 0.2\n",
    "y = pd.Series(y, name='outcome')\n",
    "\n",
    "# True CATE for this DGP: 2.5 + np.cos(X['feat_0']*np.pi) + 1.5*X['feat_3']\n",
    "true_cate = 2.5 + np.cos(X['feat_0']*np.pi) + 1.5*X['feat_3']\n",
    "true_ate = np.mean(true_cate)\n",
    "print(f\"True ATE (for generated data): {true_ate:.4f}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, treat_train, treat_test, y_train, y_test, true_cate_train, true_cate_test = train_test_split(\n",
    "    X, treatment, y, true_cate, test_size=0.3, random_state=123#, stratify=treatment # Stratify might be tricky if groups small\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Initialize the Learner ---\n",
    "pairnet_nn = PairNetTorch(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dims=[4], # Example architecture\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.0001,\n",
    "    epochs=100, # More epochs often needed\n",
    "    batch_size=256, # Larger batch size can be more stable\n",
    "    distance_threshold=2.0, # Adjust based on data scaling/distribution\n",
    "    num_neighbours=3,\n",
    "    val_size=0.15, # Use 15% of pairs for validation/early stopping\n",
    "    patience=10,\n",
    "    verbose=100, # Print progress every 10 epochs\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- 3. Fit the Model ---\n",
    "print(\"\\nFitting PairNetTorch...\")\n",
    "pairnet_nn.fit(X_train, treat_train, y_train)\n",
    "print(\"Fitting complete.\")\n",
    "\n",
    "\n",
    "# --- 4. Predict CATE ---\n",
    "print(\"\\nPredicting CATE on test set...\")\n",
    "cate_pred_nn = pairnet_nn.predict(X_test)\n",
    "print(f\"Predicted CATE shape: {cate_pred_nn.shape}\")\n",
    "print(f\"Predicted CATE for first 5 test samples:\\n{cate_pred_nn[:5].flatten()}\") # Flatten for cleaner print\n",
    "\n",
    "# Evaluate CATE prediction accuracy (e.g., R-squared or MAE vs true CATE)\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "r2 = r2_score(true_cate_test, cate_pred_nn)\n",
    "mae = mean_absolute_error(true_cate_test, cate_pred_nn)\n",
    "print(f\"CATE Prediction R2 score: {r2:.4f}\")\n",
    "print(f\"CATE Prediction MAE: {mae:.4f}\")\n",
    "\n",
    "\n",
    "# Predict components (optional)\n",
    "# cate_pred_comp_nn, y0_pred_nn, y1_pred_nn = pairnet_nn.predict(X_test, return_components=True)\n",
    "# print(f\"\\nPredicted y0 for first 5 test samples:\\n{y0_pred_nn[:5].flatten()}\")\n",
    "# print(f\"Predicted y1 for first 5 test samples:\\n{y1_pred_nn[:5].flatten()}\")\n",
    "\n",
    "\n",
    "# --- 5. Estimate ATE ---\n",
    "print(\"\\nEstimating ATE on test set...\")\n",
    "ate_pred_nn = pairnet_nn.estimate_ate(X_test)\n",
    "print(f\"Estimated ATE: {ate_pred_nn:.4f}\")\n",
    "print(f\"True ATE:      {np.mean(true_cate_test):.4f}\") # ATE on the test set specifically\n",
    "print(f\"ATE Error:     {abs(ate_pred_nn - np.mean(true_cate_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "class LassoSplineLearner:\n",
    "    def __init__(self, df_spline=7, cv=5):\n",
    "        \"\"\"\n",
    "        LASSO model with natural spline and pairwise interactions.\n",
    "        \n",
    "        :param df_spline: Degrees of freedom for the natural spline.\n",
    "        :param cv: Number of cross-validation folds for LASSO.\n",
    "        \"\"\"\n",
    "        self.df_spline = df_spline\n",
    "        self.cv = cv\n",
    "        self.lasso = None  # Model will be assigned after fitting\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.df_spline, self.cv\n",
    "    \n",
    "    def _transform_features(self, X):\n",
    "        \"\"\"Apply natural spline expansion and pairwise interactions.\"\"\"\n",
    "        df = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n",
    "        # Create spline transformations for each feature individually\n",
    "        spline_terms = []\n",
    "        for i in range(X.shape[1]):\n",
    "            spline_terms.append(f\"bs(X{i}, df={self.df_spline}, include_intercept=False)\")\n",
    "        \n",
    "        formula = \" + \".join(spline_terms)  # Combine all spline transformations\n",
    "        spline_basis = dmatrix(formula, data=df, return_type='dataframe')\n",
    "\n",
    "        # Apply pairwise interactions\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True, )\n",
    "        return poly.fit_transform(spline_basis)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit LASSO on transformed features.\"\"\"\n",
    "        X_transformed = self._transform_features(X)\n",
    "        self.lasso = Lasso(random_state=123, ).fit(X_transformed, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using trained LASSO model.\"\"\"\n",
    "        if self.lasso is None:\n",
    "            raise ValueError(\"Model is not trained. Call `.fit()` first.\")\n",
    "        X_transformed = self._transform_features(X)\n",
    "        return self.lasso.predict(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, learner):\n",
    "    learner_dict = {\n",
    "        'S-Learner': BaseSRegressor(learner),\n",
    "        'T-Learner': BaseTRegressor(learner),\n",
    "        'X-Learner': BaseXRegressor(learner),\n",
    "        'RT-Learner': BaseRTRegressor(learner),\n",
    "        'P-Learner': BasePRegressor(learner),\n",
    "        'RP-Learner': BaseRPRegressor(learner)\n",
    "    }\n",
    "    return learner_dict[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_hp(learner, X_train, W_train, y_train, e_hat_train):\n",
    "\n",
    "    # 1. Define the Hyperparameter Grid\n",
    "    param_grid = {\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "        'colsample_bytree': [0.6, 0.8, 1],\n",
    "        'eta': [5e-3, 1e-2, 1.5e-2, 2.5e-2, 5e-2, 8e-2, 1e-1, 2e-1],\n",
    "        'max_depth': list(range(3, 21)),\n",
    "        'gamma': lambda: np.random.uniform(0, 0.2),\n",
    "        'min_child_weight': list(range(1, 21)),\n",
    "        'max_delta_step': list(range(1, 11)),\n",
    "    }\n",
    "\n",
    "    # 2. Generate 10 Random Hyperparameter Combinations\n",
    "    def generate_random_params(param_grid):\n",
    "        params = {}\n",
    "        for key, values in param_grid.items():\n",
    "            if callable(values):\n",
    "                params[key] = values()\n",
    "            else:\n",
    "                params[key] = np.random.choice(values)\n",
    "        return params\n",
    "\n",
    "    random_params = [generate_random_params(param_grid) for _ in range(10)]\n",
    "\n",
    "    # 3. Cross-Validation and Early Stopping\n",
    "    def evaluate_params(params, X, t, y):\n",
    "        xgb_model = xgb.XGBRegressor(**params) # or XGBClassifier, depending on your task.\n",
    "        model = deepcopy(get_model(learner, xgb_model))\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42) # 5 fold cross validation.\n",
    "        errors = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            t_train, t_test = t[train_index], t[test_index]\n",
    "            e_train, e_test = e_hat_train[train_index], e_hat_train[test_index]\n",
    "\n",
    "            model.fit(X_train, t_train, y_train, p=e_train)\n",
    "            errors.append(mean_squared_error(model.predict(X_test, p=e_test), y_test))\n",
    "\n",
    "        return -np.mean(errors)\n",
    "    \n",
    "    # 4. Main Loop\n",
    "    best_score = float('-inf')\n",
    "    best_params = None\n",
    "\n",
    "    count = 0\n",
    "    for params in random_params:\n",
    "        count += 1\n",
    "        # print(learner, count)\n",
    "        score = evaluate_params(params, X_train, W_train, y_train) # X_train and y_train are your data.\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "\n",
    "    # print(\"Best Score:\", best_score)\n",
    "    # print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    # 5. retrain the best model.\n",
    "    best_xgb = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "    return best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairnet_nn(input_dim, hidden_dims, epochs):\n",
    "    pairnet_nn = PairNetTorch(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims, # Example architecture\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,\n",
    "        epochs=epochs, # More epochs often needed\n",
    "        batch_size=256, # Larger batch size can be more stable\n",
    "        distance_threshold=2.0, # Adjust based on data scaling/distribution\n",
    "        num_neighbours=3,\n",
    "        pairing_method='knn',\n",
    "        val_size=0.15, # Use 15% of pairs for validation/early stopping\n",
    "        patience=10,\n",
    "        verbose=epochs+1,\n",
    "    )\n",
    "    return pairnet_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_list, p_list, s_list, m_list, learner_dict, num_iter,\n",
    "                    propensity_learner=None):\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "\n",
    "        for n, p, s, m in product(n_list, p_list, s_list, m_list):\n",
    "\n",
    "            # X, W, y, tau = generate_data_setup_a(n*2, p, s)\n",
    "\n",
    "            y, X, W, tau, _, _ = synthetic_data(mode=m, n=n*2, p=p, sigma=s)\n",
    "            X_train, X_test, W_train, _, y_train, _, _, tau_test = train_test_split(\n",
    "                X, W, y, tau, test_size=0.5, random_state=111)\n",
    "\n",
    "            if propensity_learner is not None:\n",
    "                em = clone(propensity_learner)\n",
    "                em.fit(X_train, W_train)\n",
    "                e_hat_train = cross_val_predict(em, X_train, W_train, method='predict_proba')[:, 1]\n",
    "                e_hat_test = em.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            for learner in learner_dict.keys():\n",
    "                #model = deepcopy(get_model(learner, xgb_hp(learner, X_train, W_train, y_train, e_hat_train)))\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "\n",
    "                if learner != 'PairNet':\n",
    "                    model = deepcopy(learner_dict[learner])\n",
    "                    model.fit(X=X_train, treatment=W_train, y=y_train, p=e_hat_train)\n",
    "                    hat_tau = model.predict(X_test, p=e_hat_test)\n",
    "                else:\n",
    "                    model = create_pairnet_nn(X_train.shape[1], (16,16), 25)\n",
    "                    model.fit(X_train, W_train, y_train)\n",
    "                    hat_tau = model.predict(X_test)\n",
    "\n",
    "                pehe = mean_squared_error(tau_test, hat_tau)\n",
    "\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "                result_list.append([n, p, s, m, learner, pehe, elapsed_time])  \n",
    "\n",
    "    cols = ['num_samples', 'num_features', 'sigma', 'sim_mode', 'learner', 'pehe', 'time']\n",
    "    df_res = pd.DataFrame(result_list, columns=cols)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:32<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Simulation params from Nie and Wager (2020)\n",
    "n_list = [1000]\n",
    "p_list = [6]\n",
    "s_list = [0.5, 1, 2, 4]\n",
    "s_list = [0]\n",
    "m_list = [1,2,3,4]\n",
    "num_iter = 10\n",
    "\n",
    "learner=RandomForestRegressor(max_depth=20, n_estimators=100)\n",
    "learner = MLPRegressor((16,16), max_iter=100, early_stopping=True)\n",
    "#learner=Lasso()\n",
    "\n",
    "learner_dict = {\n",
    "    'S-Learner': BaseSRegressor(learner),\n",
    "    'T-Learner': BaseTRegressor(learner),\n",
    "    'X-Learner': BaseXRegressor(learner),\n",
    "    #'R-learner': BaseRRegressor(learner),\n",
    "    #'DR-learner': BaseDRRegressor(learner),\n",
    "    #'PDR-learner': BasePDRRegressor(learner),\n",
    "    #'RT-Learner': BaseRTRegressor(learner=learner),\n",
    "    'P-Learner': BasePRegressor(learner),\n",
    "    #'RP-Learner': BaseRPRegressor(learner=learner),\n",
    "    'PairNet': ''\n",
    "}\n",
    "\n",
    "propensity_learner = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "df_res_lasso = run_experiments(n_list, p_list, s_list, m_list, learner_dict, num_iter, propensity_learner=propensity_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0cee4_row0_col4, #T_0cee4_row1_col4, #T_0cee4_row2_col8, #T_0cee4_row3_col4 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0cee4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0cee4_level0_col0\" class=\"col_heading level0 col0\" >num_samples</th>\n",
       "      <th id=\"T_0cee4_level0_col1\" class=\"col_heading level0 col1\" >num_features</th>\n",
       "      <th id=\"T_0cee4_level0_col2\" class=\"col_heading level0 col2\" >sigma</th>\n",
       "      <th id=\"T_0cee4_level0_col3\" class=\"col_heading level0 col3\" >sim_mode</th>\n",
       "      <th id=\"T_0cee4_level0_col4\" class=\"col_heading level0 col4\" >P-Learner</th>\n",
       "      <th id=\"T_0cee4_level0_col5\" class=\"col_heading level0 col5\" >PairNet</th>\n",
       "      <th id=\"T_0cee4_level0_col6\" class=\"col_heading level0 col6\" >S-Learner</th>\n",
       "      <th id=\"T_0cee4_level0_col7\" class=\"col_heading level0 col7\" >T-Learner</th>\n",
       "      <th id=\"T_0cee4_level0_col8\" class=\"col_heading level0 col8\" >X-Learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0cee4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0cee4_row0_col0\" class=\"data row0 col0\" >1000</td>\n",
       "      <td id=\"T_0cee4_row0_col1\" class=\"data row0 col1\" >6</td>\n",
       "      <td id=\"T_0cee4_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_0cee4_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_0cee4_row0_col4\" class=\"data row0 col4\" >0.019209</td>\n",
       "      <td id=\"T_0cee4_row0_col5\" class=\"data row0 col5\" >0.037379</td>\n",
       "      <td id=\"T_0cee4_row0_col6\" class=\"data row0 col6\" >0.065658</td>\n",
       "      <td id=\"T_0cee4_row0_col7\" class=\"data row0 col7\" >0.104250</td>\n",
       "      <td id=\"T_0cee4_row0_col8\" class=\"data row0 col8\" >0.054618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0cee4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0cee4_row1_col0\" class=\"data row1 col0\" >1000</td>\n",
       "      <td id=\"T_0cee4_row1_col1\" class=\"data row1 col1\" >6</td>\n",
       "      <td id=\"T_0cee4_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_0cee4_row1_col3\" class=\"data row1 col3\" >2</td>\n",
       "      <td id=\"T_0cee4_row1_col4\" class=\"data row1 col4\" >0.043600</td>\n",
       "      <td id=\"T_0cee4_row1_col5\" class=\"data row1 col5\" >0.067524</td>\n",
       "      <td id=\"T_0cee4_row1_col6\" class=\"data row1 col6\" >0.431755</td>\n",
       "      <td id=\"T_0cee4_row1_col7\" class=\"data row1 col7\" >0.171362</td>\n",
       "      <td id=\"T_0cee4_row1_col8\" class=\"data row1 col8\" >0.058651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0cee4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0cee4_row2_col0\" class=\"data row2 col0\" >1000</td>\n",
       "      <td id=\"T_0cee4_row2_col1\" class=\"data row2 col1\" >6</td>\n",
       "      <td id=\"T_0cee4_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_0cee4_row2_col3\" class=\"data row2 col3\" >3</td>\n",
       "      <td id=\"T_0cee4_row2_col4\" class=\"data row2 col4\" >0.093112</td>\n",
       "      <td id=\"T_0cee4_row2_col5\" class=\"data row2 col5\" >0.297903</td>\n",
       "      <td id=\"T_0cee4_row2_col6\" class=\"data row2 col6\" >0.111171</td>\n",
       "      <td id=\"T_0cee4_row2_col7\" class=\"data row2 col7\" >0.301452</td>\n",
       "      <td id=\"T_0cee4_row2_col8\" class=\"data row2 col8\" >0.061255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0cee4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0cee4_row3_col0\" class=\"data row3 col0\" >1000</td>\n",
       "      <td id=\"T_0cee4_row3_col1\" class=\"data row3 col1\" >6</td>\n",
       "      <td id=\"T_0cee4_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_0cee4_row3_col3\" class=\"data row3 col3\" >4</td>\n",
       "      <td id=\"T_0cee4_row3_col4\" class=\"data row3 col4\" >0.008937</td>\n",
       "      <td id=\"T_0cee4_row3_col5\" class=\"data row3 col5\" >0.068574</td>\n",
       "      <td id=\"T_0cee4_row3_col6\" class=\"data row3 col6\" >0.262700</td>\n",
       "      <td id=\"T_0cee4_row3_col7\" class=\"data row3 col7\" >0.121401</td>\n",
       "      <td id=\"T_0cee4_row3_col8\" class=\"data row3 col8\" >0.172321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x3306f3490>"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot table to restructure the dataframe\n",
    "df_pivot = df_res_lasso.pivot_table(\n",
    "    index=['num_samples', 'num_features', 'sigma', 'sim_mode',],  # Grouping columns\n",
    "    columns=\"learner\",          # Learner categories become new columns\n",
    "    values=\"pehe\",               # MSE values\n",
    "    aggfunc=\"median\"              # Averaging the MSE values\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns if needed\n",
    "df_pivot.columns.name = None  # Remove the automatic column name\n",
    "\n",
    "# Function to apply bold formatting\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return [\"font-weight: bold\" if v else \"\" for v in is_min]\n",
    "\n",
    "# Apply formatting to only the learner columns\n",
    "learner_columns = df_pivot.columns[4:]  # Excluding 'n', 'd', 'sigma'\n",
    "styled_df = df_pivot.sort_values(['sim_mode', 'num_features','sigma']).style.apply(highlight_min, subset=learner_columns, axis=1)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation_descs = {\n",
    "    1: 'Difficult nuisance and easy treatment',\n",
    "    2: 'Randomized trial',\n",
    "    3: 'Easy propensity and a difficult baseline',\n",
    "    4: 'Unrelated treatment and control'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, m in zip(range(4), m_list):\n",
    "    sns.boxplot(x='learner', y='pehe', data=df_res_lasso.loc[(df_res_lasso['sim_mode'] == m)], linewidth=1, showfliers=False, ax=axs[i], palette=palette)\n",
    "    axs[i].title.set_text(data_generation_descs[m] + ' (RF)')\n",
    "    axs[i].set_ylabel('PEHE')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(labelsize=12)\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
